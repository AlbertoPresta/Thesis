{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c65ed1ef79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distance'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import cv2\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER CLASS \n",
    "\n",
    "Define a class which represents the main block of the prototypical networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional Tensor of shape [batch_size, d1, d2, ..., dn] to 2-dimensional Tensor\n",
    "    of shape [batch_size, d1*d2*...*dn].\n",
    "    # Arguments\n",
    "        input: Input tensor\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                         nn.BatchNorm2d(out_channels),nn.ReLU(),nn.MaxPool2d(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_protonet_conv(x_dim, hid_dim, z_dim):\n",
    "    \"\"\"\n",
    "    Loads the prototypical network model\n",
    "    Arg:\n",
    "    x_dim (tuple): dimension of input image\n",
    "    hid_dim (int): dimension of hidden layers in conv blocks\n",
    "    z_dim (int): dimension of embedded image\n",
    "    Returns:\n",
    "    Model (Class ProtoNet)\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    encoder = nn.Sequential(conv_block(x_dim[0], hid_dim),conv_block(hid_dim, hid_dim),\n",
    "                            conv_block(hid_dim, hid_dim),conv_block(hid_dim, z_dim),Flatten())\n",
    "    \n",
    "    return Protonet(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROTONET CLASS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Protonet(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def set_forward_loss(self, sample):\n",
    "        \"\"\"\n",
    "        Computes loss, accuracy and output for classification task\n",
    "        Args:\n",
    "            sample (torch.Tensor): shape (n_way, n_support+n_query, (dim)) \n",
    "        Returns:\n",
    "            torch.Tensor: shape(2), loss, accuracy and y_hat\n",
    "        \"\"\"\n",
    "        sample_images = sample['images']\n",
    "        n_way = sample['n_way']\n",
    "        n_support = sample['n_support']\n",
    "        n_query = sample['n_query']\n",
    "\n",
    "        x_support = sample_images[:, :n_support]\n",
    "        x_query = sample_images[:, n_support:]\n",
    "\n",
    "        #target indices are 0 ... n_way-1\n",
    "        #target_inds = TENSOR WHICH HAS dimension (n_way,n_query,1), which represent the arange from 0 to n_way of a matrix \n",
    "        #of dimension (n_query,1) (c[0]= [[0,0,0]])\n",
    "        target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()\n",
    "        target_inds = Variable(target_inds, requires_grad=False)\n",
    "\n",
    "        #encode images of the support and the query set\n",
    "        x = torch.cat([x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]),\n",
    "                       x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])], 0)\n",
    "\n",
    "        z = self.encoder.forward(x)\n",
    "        z_dim = z.size(-1) #usually 64\n",
    "        z_proto = z[:n_way*n_support].view(n_way, n_support, z_dim).mean(1)\n",
    "        z_query = z[n_way*n_support:]\n",
    "\n",
    "        #compute distances\n",
    "        dists = distance.euclidean_dist(z_query, z_proto)\n",
    "\n",
    "        #compute probabilities\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_way, n_query, -1)\n",
    "\n",
    "        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "        _, y_hat = log_p_y.max(2)\n",
    "        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "\n",
    "        df = {'loss': loss_val.item(),'acc': acc_val.item(),'y_hat': y_hat}\n",
    "        return loss_val, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda10374d56d606404d89697440b2a570f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
