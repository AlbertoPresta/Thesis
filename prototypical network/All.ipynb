{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_alphabets(alphabet_directory_path, alphabet_directory_name):\n",
    "    \"\"\"\n",
    "    Reads all the characters from a given alphabet_directory\n",
    "    \"\"\"\n",
    "    datax = []\n",
    "    datay = []\n",
    "    characters = os.listdir(alphabet_directory_path)\n",
    "    for character in characters:\n",
    "        images = os.listdir(alphabet_directory_path + character + '/')\n",
    "        for img in images:\n",
    "            print(alphabet_directory_path + character + '/' + img)\n",
    "            image = cv2.resize(cv2.imread(alphabet_directory_path + character + '/' + img),(500,500))\n",
    "            #rotations of image\n",
    "            rotated_90 = ndimage.rotate(image, 90)\n",
    "            rotated_180 = ndimage.rotate(image, 180)\n",
    "            rotated_270 = ndimage.rotate(image, 270)\n",
    "            # crop image \n",
    "\n",
    "            print(image.shape)\n",
    "            #start_y = random.randint(0, image.shape[0])\n",
    "            #dim_y = random.randint(0,image.shape[0])\n",
    "            #start_x = random.randint(0,image.shape[1])\n",
    "            #dim_x = random.randint(0,image.shape[1])\n",
    "            #temp = image[start_y:min(start_y+dim_y,image.shape[0] - 1),start_x:min(start_x+dim_x,image.shape[1]-1),:]\n",
    "            #temp = cv2.resize(temp,(image.shape[1],image.shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "            # end crop image\n",
    "            datax.extend((image, rotated_90, rotated_180, rotated_270))\n",
    "            datay.extend((character,character,character,character))\n",
    "    return np.array(datax), np.array(datay)\n",
    "\n",
    "def read_images(base_directory):\n",
    "    \"\"\"\n",
    "    Reads all the alphabets from the base_directory\n",
    "    \"\"\"\n",
    "    datax = None\n",
    "    datay = None\n",
    "    results = []\n",
    "    for directory in os.listdir(base_directory):\n",
    "        results.append(read_alphabets(base_directory + '/' + directory + '/',directory))\n",
    "    for result in results:\n",
    "        if datax is None:\n",
    "            datax = result[0]\n",
    "            datay = result[1]\n",
    "        else:\n",
    "            datax = np.vstack([datax, result[0]])\n",
    "            datay = np.concatenate([datay, result[1]])\n",
    "    return datax, datay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_sample(n_way, n_support, n_query, datax, datay):\n",
    "    sample = []\n",
    "    unique_y = np.unique(datay)\n",
    "    K = np.random.choice(unique_y, n_way, replace = False)\n",
    "    for cls in K:\n",
    "        datax_cls = datax[datay==cls]\n",
    "        perm = np.random.permutation(datax_cls)\n",
    "        sample_cls = perm[:(n_support + n_query)]\n",
    "        sample.append(sample_cls)\n",
    "    # sample in the end will be a matrix of dimension  k X n_support + n_query\n",
    "    \n",
    "    sample = np.array(sample) #become a np array of array (matrix)\n",
    "    sample = torch.from_numpy(sample).float() # become a tensor\n",
    "    sample = sample.permute(0,1,4,2,3) \n",
    "    df = {'images': sample, 'n_way': n_way,'n_support': n_support,'n_query': n_query}\n",
    "    return df\n",
    "\n",
    "\n",
    "def display_sample(sample):\n",
    "    \"\"\"\n",
    "    Displays sample in a grid\n",
    "    Args:\n",
    "    sample (torch.Tensor): sample of images to display\n",
    "    \"\"\"\n",
    "    #need 4D tensor to create grid, currently 5D\n",
    "    sample_4D = sample.view(sample.shape[0]*sample.shape[1],*sample.shape[2:])\n",
    "    #make a grid\n",
    "    out = torchvision.utils.make_grid(sample_4D, nrow=sample.shape[1])\n",
    "    plt.figure(figsize = (16,7))\n",
    "    plt.imshow(out.permute(1, 2, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    \"\"\"\n",
    "    Computes euclidean distance btw x and y\n",
    "    Args:\n",
    "        x (torch.Tensor): shape (n, d). n usually n_way*n_query\n",
    "        y (torch.Tensor): shape (m, d). m usually n_way\n",
    "    Returns:\n",
    "        torch.Tensor: shape(n, m). For each query, the distances to each centroid\n",
    "    \"\"\"\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_x, train_y, n_way, n_support, n_query, max_epoch,epoch_size,PATH = \"model/protonet.pt\"):\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,1,gamma = 0.5, last_epoch = -1)\n",
    "    epoch = 0\n",
    "    while(epoch < max_epoch):\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        \n",
    "        for episode in tqdm(range(epoch_size)):\n",
    "            sample = extract_sample(n_way, n_support, n_query, train_x, train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = model.set_forward_loss(sample)\n",
    "            running_loss += output['loss']\n",
    "            running_acc += output['acc']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss = running_loss / epoch_size\n",
    "        epoch_acc = running_acc / epoch_size\n",
    "        print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,epoch_loss, epoch_acc))\n",
    "        epoch += 1\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def test(model, test_x, test_y, n_way, n_support, n_query, test_episode):\n",
    "    \"\"\"\n",
    "    Tests the protonet\n",
    "    Args:\n",
    "        model: trained model\n",
    "        test_x (np.array): images of testing set\n",
    "        test_y (np.array): labels of testing set\n",
    "        n_way (int): number of classes in a classification task\n",
    "        n_support (int): number of labeled examples per class in the support set\n",
    "        n_query (int): number of labeled examples per class in the query set\n",
    "        test_episode (int): number of episodes to test on\n",
    "      \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for episode in tqdm(range(test_episode)):\n",
    "        sample = extract_sample(n_way, n_support, n_query, test_x, test_y)\n",
    "        loss, output = model.set_forward_loss(sample)\n",
    "        running_loss += output['loss']\n",
    "        running_acc += output['acc']\n",
    "    avg_loss = running_loss / test_episode\n",
    "    avg_acc = running_acc / test_episode\n",
    "    print('Test results -- Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, avg_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROTONET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional Tensor of shape [batch_size, d1, d2, ..., dn] to 2-dimensional Tensor\n",
    "    of shape [batch_size, d1*d2*...*dn].\n",
    "    # Arguments\n",
    "        input: Input tensor\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    \n",
    "    \n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                         nn.BatchNorm2d(out_channels),nn.ReLU(),nn.MaxPool2d(2))\n",
    "\n",
    "def load_protonet_conv(x_dim, hid_dim, z_dim):\n",
    "    \"\"\"\n",
    "    Loads the prototypical network model\n",
    "    Arg:\n",
    "    x_dim (tuple): dimension of input image\n",
    "    hid_dim (int): dimension of hidden layers in conv blocks\n",
    "    z_dim (int): dimension of embedded image\n",
    "    Returns:\n",
    "    Model (Class ProtoNet)\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    encoder = nn.Sequential(conv_block(x_dim[0], hid_dim),conv_block(hid_dim, hid_dim),\n",
    "                            conv_block(hid_dim, hid_dim),conv_block(hid_dim, z_dim),Flatten())\n",
    "    \n",
    "    return Protonet(encoder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Protonet(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def set_forward_loss(self, sample):\n",
    "        \"\"\"\n",
    "        Computes loss, accuracy and output for classification task\n",
    "        Args:\n",
    "            sample (torch.Tensor): shape (n_way, n_support+n_query, (dim)) \n",
    "        Returns:\n",
    "            torch.Tensor: shape(2), loss, accuracy and y_hat\n",
    "        \"\"\"\n",
    "        sample_images = sample['images']\n",
    "        n_way = sample['n_way']\n",
    "        n_support = sample['n_support']\n",
    "        n_query = sample['n_query']\n",
    "\n",
    "        x_support = sample_images[:, :n_support]\n",
    "        x_query = sample_images[:, n_support:]\n",
    "\n",
    "        #target indices are 0 ... n_way-1\n",
    "        #target_inds = TENSOR WHICH HAS dimension (n_way,n_query,1), which represent the arange from 0 to n_way of a matrix \n",
    "        #of dimension (n_query,1) (c[0]= [[0,0,0]])\n",
    "        target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()\n",
    "        target_inds = Variable(target_inds, requires_grad=False)\n",
    "\n",
    "        #encode images of the support and the query set\n",
    "        x = torch.cat([x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]),\n",
    "                       x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])], 0)\n",
    "\n",
    "        z = self.encoder.forward(x)\n",
    "        z_dim = z.size(-1) #usually 64\n",
    "        z_proto = z[:n_way*n_support].view(n_way, n_support, z_dim).mean(1)\n",
    "        z_query = z[n_way*n_support:]\n",
    "\n",
    "        #compute distances\n",
    "        dists = euclidean_dist(z_query, z_proto)\n",
    "\n",
    "        #compute probabilities\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_way, n_query, -1)\n",
    "\n",
    "        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "        _, y_hat = log_p_y.max(2)\n",
    "        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "\n",
    "        df = {'loss': loss_val.item(),'acc': acc_val.item(),'y_hat': y_hat}\n",
    "        return loss_val, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "trainx, trainy = read_images('../Licheni')\n",
    "testx, testy = read_images('../lich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = load_protonet_conv(x_dim=(3,28,28),hid_dim=64,z_dim=64)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "n_way = 3\n",
    "n_support = 2\n",
    "n_query = 2\n",
    "\n",
    "train_x = trainx\n",
    "train_y = trainy\n",
    "\n",
    "max_epoch = 5\n",
    "epoch_size = 100\n",
    "\n",
    "train(model, optimizer, train_x, train_y, n_way, n_support, n_query, max_epoch, epoch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 3\n",
    "n_support = 2\n",
    "n_query = 2\n",
    "testx, testy = read_images('../lich')\n",
    "test_x = testx\n",
    "test_y = testy\n",
    "\n",
    "test_episode = 100\n",
    "\n",
    "test(model, test_x, test_y, n_way, n_support, n_query, test_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ON SPECIFIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 3\n",
    "n_support = 4\n",
    "n_query = 4\n",
    "my_sample = extract_sample(n_way, n_support, n_query, test_x, test_y)\n",
    "display_sample(my_sample['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_loss, my_output = model.set_forward_loss(my_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda10374d56d606404d89697440b2a570f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
