{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Input, Subtract, Lambda\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import numpy.random as rng\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "import lichensloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SiameseNetwork:\n",
    "    \"\"\"\n",
    "    Construction of the network for training.\n",
    "    \n",
    "    Attributes:\n",
    "        input_shape: image size \n",
    "        model : current siamese model \n",
    "        learning_rate: SGD learning rate \n",
    "        summary_writer: tensorflow writer to store the logs \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, learning_rate, batch_size, use_augmentation, \n",
    "                 learning_rate_multipliers, l2_regularization_penalization):\n",
    "        \n",
    "        self.input_shape = (400,400,3)\n",
    "        self.model = []\n",
    "        self.learning_rate = learning_rate \n",
    "        #self.summary_writer = tf.summary.FileWriter(tensorboard_log_path)\n",
    "        \n",
    "        self._construct_siamese_architecture(learning_rate_multipliers, l2_regularization_penalization)\n",
    "        \n",
    "        self.lichen_loader = lichensloader.Lichensloader(dataset_path=dataset_path, use_augmentation=use_augmentation, batch_size=batch_size)\n",
    "    \n",
    "    def contrastive_loss(self,y_true, y_pred):\n",
    "        '''Contrastive loss from Hadsell-et-al.'06\n",
    "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "        '''\n",
    "        margin = 1\n",
    "        sqaure_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "    def _construct_siamese_architecture(self, learning_rate_multipliers, l2_regularization_penalization):\n",
    "        \"\"\"\n",
    "        Costruct the siamese netwrok architecture and stores it in the class\n",
    "        \n",
    "        Arguments:\n",
    "            learning_rate_multipliers\n",
    "            l2_regularization_penalization\n",
    "        \"\"\"\n",
    "    \n",
    "        convolutional_net = Sequential()\n",
    "        convolutional_net.add(Conv2D(filters = 64, kernel_size = (10,10),activation = 'relu',\n",
    "                              input_shape = self.input_shape, kernel_regularizer = l2(l2_regularization_penalization['Conv1']),\n",
    "                              name = 'Conv1'))\n",
    "        \n",
    "        convolutional_net.add(MaxPool2D())\n",
    "        \n",
    "        \n",
    "        \n",
    "        convolutional_net.add(Conv2D(filters = 128, kernel_size = (7,7),activation = 'relu',\n",
    "                              input_shape = self.input_shape, kernel_regularizer = l2(l2_regularization_penalization['Conv2']),\n",
    "                              name = 'Conv2'))\n",
    "        \n",
    "        convolutional_net.add(MaxPool2D())\n",
    "        \n",
    "        \n",
    "        \n",
    "        convolutional_net.add(Conv2D(filters = 128, kernel_size = (4,4),activation = 'relu',\n",
    "                              input_shape = self.input_shape, kernel_regularizer = l2(l2_regularization_penalization['Conv3']),\n",
    "                              name = 'Conv3'))\n",
    "        \n",
    "        convolutional_net.add(MaxPool2D())\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        convolutional_net.add(Conv2D(filters = 256, kernel_size = (4,4),activation = 'relu',\n",
    "                              input_shape = self.input_shape, kernel_regularizer = l2(l2_regularization_penalization['Conv4']),\n",
    "                              name = 'Conv4'))\n",
    "        \n",
    "        convolutional_net.add(MaxPool2D())\n",
    "        \n",
    "        \n",
    "        convolutional_net.add(Flatten())\n",
    "        \n",
    "        \n",
    "        convolutional_net.add(Dense(units = 4096, activation = 'sigmoid',kernel_regularizer = l2(\n",
    "                                l2_regularization_penalization['Dense1']),name = 'Dense1'))\n",
    "        \n",
    "        \n",
    "        input_image_1 = Input(self.input_shape)\n",
    "        input_image_2 = Input(self.input_shape)\n",
    "        \n",
    "        \n",
    "        encoded_image_1 = convolutional_net(input_image_1)\n",
    "        encoded_image_2 = convolutional_net(input_image_2)\n",
    "        \n",
    "        l1_distance_layer = Lambda( lambda tensors : K.abs(tensors[0] - tensors[1]))\n",
    "        \n",
    "        l1_distance = l1_distance_layer([encoded_image_1, encoded_image_2])\n",
    "        \n",
    "        # Prediction layers \n",
    "        prediction = Dense(units = 1 , activation = 'sigmoid')(l1_distance)\n",
    "        \n",
    "        self.model = Model(inputs = [input_image_1,input_image_2],outputs = prediction)\n",
    "        \n",
    "        \n",
    "        optimizer = Adam(lr = 0.00006)\n",
    "        \n",
    "        self.model.compile(loss = self.contrastive_loss, metrics = ['binary_accuracy'],\n",
    "                           optimizer = optimizer)\n",
    "        \n",
    "        \n",
    "\n",
    "    def train_siamese_network(self, number_of_iterations, support_set_size,\n",
    "                              final_momentum, momentum_slope, evaluate_each,\n",
    "                              model_name):\n",
    "        \"\"\" Train the Siamese net\n",
    "        This is the main function for training the siamese net. \n",
    "        In each every evaluate_each train iterations we evaluate one-shot tasks in \n",
    "        validation and evaluation set. We also write to the log file.\n",
    "        Arguments:\n",
    "            number_of_iterations: maximum number of iterations to train.\n",
    "            support_set_size: number of characters to use in the support set\n",
    "                in one-shot tasks.\n",
    "            final_momentum: mu_j in the paper. Each layer starts at 0.5 momentum\n",
    "                but evolves linearly to mu_j\n",
    "            momentum_slope: slope of the momentum evolution. In the paper we are\n",
    "                only told that this momentum evolves linearly. Because of that I \n",
    "                defined a slope to be passed to the training.\n",
    "            evaluate each: number of iterations defined to evaluate the one-shot\n",
    "                tasks.\n",
    "            model_name: save_name of the model\n",
    "        Returns: \n",
    "            Evaluation Accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # First of all let's divide randomly the 30 train alphabets in train\n",
    "        # and validation with 24 for training and 6 for validation\n",
    "        self.lichen_loader.split_train_datasets()\n",
    "\n",
    "        # Variables that will store 100 iterations losses and accuracies\n",
    "        # after evaluate_each iterations these will be passed to tensorboard logs\n",
    "        train_losses = np.zeros(shape=(evaluate_each))\n",
    "        train_accuracies = np.zeros(shape=(evaluate_each))\n",
    "        count = 0\n",
    "        earrly_stop = 0\n",
    "        # Stop criteria variables\n",
    "        best_validation_accuracy = 0.0\n",
    "        best_accuracy_iteration = 0\n",
    "        validation_accuracy = 0.0\n",
    "\n",
    "\n",
    "        # Train loop\n",
    "        for iteration in range(number_of_iterations):\n",
    "            print(\"----> \",iteration )\n",
    "\n",
    "            # train set\n",
    "            images, labels = self.lichen_loader.create_pairs_for_batch()\n",
    "            train_loss, train_accuracy = self.model.train_on_batch(\n",
    "                images, labels)\n",
    "\n",
    "            # Decay learning rate 1 % per 500 iterations (in the paper the decay is\n",
    "            # 1% per epoch). Also update linearly the momentum (starting from 0.5 to 1)\n",
    "            if (iteration + 1) % 500 == 0:\n",
    "                K.set_value(self.model.optimizer.lr, K.get_value(\n",
    "                    self.model.optimizer.lr) * 0.99)\n",
    "            if K.get_value(self.model.optimizer.momentum) < final_momentum:\n",
    "                K.set_value(self.model.optimizer.momentum, K.get_value(\n",
    "                    self.model.optimizer.momentum) + momentum_slope)\n",
    "\n",
    "            train_losses[count] = train_loss\n",
    "            train_accuracies[count] = train_accuracy\n",
    "\n",
    "            # validation set\n",
    "            count += 1\n",
    "            print('Iteration %d/%d: Train loss: %f, Train Accuracy: %f, lr = %f' %\n",
    "                  (iteration + 1, number_of_iterations, train_loss, train_accuracy, K.get_value(\n",
    "                      self.model.optimizer.lr)))\n",
    "\n",
    "            # Each 100 iterations perform a one_shot_task and write to tensorboard the\n",
    "            # stored losses and accuracies\n",
    "            if (iteration + 1) % evaluate_each == 0:\n",
    "                print(\"******\",number_of_runs_per_alphabet)\n",
    "                number_of_runs_per_alphabet = 40\n",
    "                # use a support set size equal to the number of character in the alphabet\n",
    "                validation_accuracy = self.lichen_loader.one_shot_test(\n",
    "                    self.model, support_set_size, number_of_runs_per_alphabet, is_validation=True)\n",
    "\n",
    "                #self._write_logs_to_tensorboard(\n",
    "                #    iteration, train_losses, train_accuracies,\n",
    "                #   validation_accuracy, evaluate_each)\n",
    "                count = 0\n",
    "\n",
    "                # Some hyperparameters lead to 100%, although the output is almost the same in \n",
    "                # all images. \n",
    "                if (validation_accuracy == 1.0 and train_accuracy == 0.5):\n",
    "                    print('Early Stopping: Gradient Explosion')\n",
    "                    print('Validation Accuracy = ' +\n",
    "                          str(best_validation_accuracy))\n",
    "                    return 0\n",
    "                elif train_accuracy == 0.0:\n",
    "                    return 0\n",
    "                else:\n",
    "                    # Save the model\n",
    "                    if validation_accuracy > best_validation_accuracy:\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_accuracy_iteration = iteration\n",
    "                        \n",
    "                        model_json = self.model.to_json()\n",
    "\n",
    "                        if not os.path.exists('./models'):\n",
    "                            os.makedirs('./models')\n",
    "                        with open('models/' + model_name + '.json', \"w\") as json_file:\n",
    "                            json_file.write(model_json)\n",
    "                        self.model.save_weights('models/' + model_name + '.h5')\n",
    "\n",
    "            # If accuracy does not improve for 10000 batches stop the training\n",
    "            if iteration - best_accuracy_iteration > 10000:\n",
    "                print(\n",
    "                    'Early Stopping: validation accuracy did not increase for 10000 iterations')\n",
    "                print('Best Validation Accuracy = ' +\n",
    "                      str(best_validation_accuracy))\n",
    "                print('Validation Accuracy = ' + str(best_validation_accuracy))\n",
    "                break\n",
    "\n",
    "        print('Trained Ended!')\n",
    "        return best_validation_accuracy\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda10374d56d606404d89697440b2a570f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
