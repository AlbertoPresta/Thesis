{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cell dedicated to google drive if it will be necessary a GPU\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import cv2 \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, concatenate, Lambda\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from keras.models import Sequential,Model\n",
    "import time\n",
    "from keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = np.load('training_data.npy')\n",
    "train_label = np.load('training_labe.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL GENERATION \n",
    "\n",
    "Here we make feature generation network (siamese network) to process image into features. The network starts off randomly initialized and will be trained to generate useful vector features from input images (hopefully). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(input_shape): \n",
    "    \n",
    "    \n",
    "    # define two inputs \n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64, (10,10), activation='relu',kernel_initializer = 'random_normal',bias_initializer='random_normal',  \n",
    "              input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(128, (7,7), activation='relu',  kernel_initializer = 'random_normal',bias_initializer='random_normal',\n",
    "              input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "\n",
    "    model.add(Conv2D(128, (4,4), activation='relu',  kernel_initializer = 'random_normal',bias_initializer='random_normal',\n",
    "              input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer = 'random_normal',bias_initializer='random_normal',\n",
    "              input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(4096, activation='sigmoid',kernel_regularizer=l2(1e-3))) \n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    \n",
    "    combined_features = concatenate([encoded_l, encoded_r], name = 'merge_features')\n",
    "    \n",
    "\n",
    "    combined_features = Dense(16, activation = 'linear')(combined_features)\n",
    "    combined_features = BatchNormalization()(combined_features)\n",
    "    combined_features = Activation('relu')(combined_features)\n",
    "    combined_features = Dense(4, activation = 'linear')(combined_features)\n",
    "    combined_features = BatchNormalization()(combined_features)\n",
    "    combined_features = Activation('relu')(combined_features)\n",
    "    combined_features = Dense(1, activation = 'sigmoid')(combined_features)\n",
    "    similarity_model = Model(inputs = [left_input, right_input], outputs = [combined_features], name = 'Similarity_Model')\n",
    "    #similarity_model.summary()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    #L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    #prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "\n",
    "        # Connect the inputs with the outputs\n",
    "    #siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return similarity_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "input_dim = train_data.shape[2:]\n",
    "print(input_dim)\n",
    "base_network = siamese_model(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimization process\n",
    "base_network.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_label, test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (5100, 2, 100, 100, 3)\n",
      "x_test (900, 2, 100, 100, 3)\n",
      "y_train (5100, 1)\n",
      "y_test (900, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train',x_train.shape)\n",
    "print('x_test',x_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "rms = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#RMSprop()\n",
    "rms = RMSprop()\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "callback_early_stop_reduceLROnPlateau=[earlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Similarity_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 4096)         5406016     input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_features (Concatenate)    (None, 8192)         0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 16)           131088      merge_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16)           64          dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 4)            68          activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4)            16          dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4)            0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            5           activation_6[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,537,257\n",
      "Trainable params: 5,537,217\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_network.compile(loss=\"binary_crossentropy\", optimizer=rms,metrics=[\"binary_accuracy\"])\n",
    "base_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4080 samples, validate on 1020 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "img_1 = x_train[:, 0]\n",
    "img2 = x_train[:, 1]\n",
    "\n",
    "history = base_network.fit([img_1, img2], y_train, validation_split=.20,batch_size= 256, verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda10374d56d606404d89697440b2a570f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
